{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics \n",
    "### Gini impurity\n",
    "应用于CART分类决策树,Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The Gini impurity can be computed by summing the probability $p_{i}$ of an item with label i being chosen times the probability $\\sum _{k\\neq i}p_{k}=1-p_{i}$ of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category.\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e3c72a8e705c2fddf2fd552241c579a6c146af7f\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain  \n",
    "Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy and information content from information theory.   \n",
    "熵定义:${H} (T)=\\operatorname {I} _{E}\\left(p_{1},p_{2},...,p_{J}\\right)=-\\sum _{i=1}^{J}{p_{i}\\log _{2}p_{i}}$\n",
    "上述定义是对离散型随机变量的定义  \n",
    "where $\\displaystyle p_{1},p_{2},...$ $\\displaystyle p_{1},p_{2},...$are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree.  \n",
    "$\\displaystyle \\overbrace {IG(T,a)} ^{\\text{Information Gain}}=\\overbrace {\\mathrm {H} (T)} ^{\\text{Entropy (parent)}}-\\overbrace {\\mathrm {H} (T|a)} ^{\\text{Weighted Sum of Entropy (Children)}}$ $\\displaystyle =-\\sum _{i=1}^{J}p_{i}\\log _{2}{p_{i}}-\\sum _{a}{p(a)\\sum _{i=1}^{J}-\\Pr(i|a)\\log _{2}{\\Pr(i|a)}}$ $\\displaystyle =-\\sum _{i=1}^{J}p_{i}\\log _{2}{p_{i}}-\\sum _{a}{p(a)\\sum _{i=1}^{J}-\\Pr(i|a)\\log _{2}{\\Pr(i|a)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3  \n",
    "伪代码,是C4.5的前一个版本\n",
    "```\n",
    "ID3 (Examples, Target_Attribute, Attributes)\n",
    "    Create a root node for the tree\n",
    "    If all examples are positive, Return the single-node tree Root, with label = +.\n",
    "    If all examples are negative, Return the single-node tree Root, with label = -.\n",
    "    If number of predicting attributes is empty, then Return the single node tree Root,\n",
    "    with label = most common value of the target attribute in the examples.\n",
    "    Otherwise Begin\n",
    "        A ← The Attribute that best classifies examples.\n",
    "        Decision Tree attribute for Root = A.\n",
    "        For each possible value, vi, of A,\n",
    "            Add a new tree branch below Root, corresponding to the test A = vi.\n",
    "            Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "            If Examples(vi) is empty\n",
    "                Then below this new branch add a leaf node with label = most common target value in the examples\n",
    "            Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes – {A})\n",
    "    End\n",
    "    Return Root```  \n",
    "容易过拟合,不支持连续型特征,只能用于分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from scipy.stats import entropy\n",
      "from collections import Counter, defaultdict\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class ID3:\n",
      "    def __init__(self):\n",
      "        self.root = None\n",
      "        self.features = None\n",
      "\n",
      "    def fit(self, x: pd.DataFrame, y: pd.Series):\n",
      "        \"\"\"\n",
      "        训练ID3模型\n",
      "        :param x: pandas.DataFrame 训练特征数据\n",
      "        :param y: pandas.Series 目标数据\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        self.features = x.columns\n",
      "        self.root = ID3._split_data(x, y, x.columns)\n",
      "\n",
      "    def predict(self, x):\n",
      "        assert len(x.shape) == 2\n",
      "        result = []\n",
      "        for i in range(x.shape[0]):\n",
      "            result.append(ID3._look_up(x.iloc[i, :], self.root))\n",
      "        return pd.Series(result)\n",
      "\n",
      "    @staticmethod\n",
      "    def _look_up(features, node):\n",
      "        label = None\n",
      "        if \"label\" in node:\n",
      "            label = node[\"label\"]\n",
      "        else:\n",
      "            for n in node[\"nodes\"]:\n",
      "                fe_name = n[\"split_feature\"]\n",
      "                if features[fe_name] == n[\"feature_value\"]:\n",
      "                    label = ID3._look_up(features, n)\n",
      "        if label is None:\n",
      "            label = \"Lookup Fail\"\n",
      "        return label\n",
      "\n",
      "    @staticmethod\n",
      "    def _most_common_label(labels):\n",
      "        label_counter = Counter(labels)\n",
      "        return label_counter.most_common(1)[0][0]\n",
      "\n",
      "    @staticmethod\n",
      "    def _split_data(x: pd.DataFrame, y: pd.Series, remain_features):\n",
      "        node = dict()\n",
      "        # 类别唯一返回叶子节点\n",
      "        if len(y.unique()) == 1:\n",
      "            node[\"label\"] = y.unique()[0]\n",
      "            return node\n",
      "\n",
      "        # 无可分割属性，返回叶子节点以数量最多的类别作为分类结果\n",
      "        if len(remain_features) == 0:\n",
      "            node[\"label\"] = ID3._most_common_label(y)\n",
      "\n",
      "        best_feature, metric = ID3._best_split_feature(x, y, remain_features)\n",
      "        node[\"nodes\"] = list()\n",
      "\n",
      "        for value in x[best_feature].unique():\n",
      "            examples = x[x[best_feature] == value]\n",
      "            target = y[x[best_feature] == value]\n",
      "            child_node = ID3._split_data(examples, target, list([fe for fe in remain_features if fe != best_feature]))\n",
      "            child_node[\"feature_value\"] = value\n",
      "            child_node[\"split_feature\"] = best_feature\n",
      "            child_node[\"metric\"] = metric\n",
      "            node[\"nodes\"].append(child_node)\n",
      "        return node\n",
      "\n",
      "    @staticmethod\n",
      "    def _gain(x, y):\n",
      "        fe_group_feature = defaultdict(list)\n",
      "        for fe, label in zip(x, y):\n",
      "            fe_group_feature[fe].append(label)\n",
      "        after_entropy = 0\n",
      "        before_entropy = entropy(np.asarray(list(Counter(y).values())) / len(y))\n",
      "        for fe in fe_group_feature:\n",
      "            labels = fe_group_feature[fe]\n",
      "            label_counter = Counter(labels)\n",
      "            label_counts = np.asarray(list(label_counter.values()))\n",
      "            pk = label_counts / np.sum(label_counts)\n",
      "            e = entropy(pk)\n",
      "            label_portion = len(fe_group_feature[fe]) / len(y)\n",
      "            after_entropy += e * label_portion\n",
      "        return before_entropy - after_entropy\n",
      "\n",
      "    @staticmethod\n",
      "    def _best_split_feature(x, y, remain_features):\n",
      "        metric_map = dict()\n",
      "        best_feature = None\n",
      "        best_metric = None\n",
      "        for feature in remain_features:\n",
      "            metric = ID3._gain(x[feature], y)\n",
      "            metric_map[feature] = metric\n",
      "            if best_metric is None:\n",
      "                best_metric = metric\n",
      "                best_feature = feature\n",
      "            elif best_metric < metric:\n",
      "                best_metric = metric\n",
      "                best_feature = feature\n",
      "        return best_feature, best_metric\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    dataset = pd.read_csv(\"xigua3.csv\")\n",
      "    x = dataset[[\"色泽\", \"根蒂\", \"敲声\", \"纹理\", \"脐部\", \"触感\"]]\n",
      "    y = dataset[\"好瓜\"]\n",
      "    from sklearn.model_selection import train_test_split\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
      "    model = ID3()\n",
      "    model.fit(X_train, y_train)\n",
      "    print(\"实际值\", y_test)\n",
      "    pred = model.predict(X_test)\n",
      "    print(\"预测值\", pred)\n",
      "    print(\"acc:\", np.sum(pred.values == y_test.values) / len(pred))\n"
     ]
    }
   ],
   "source": [
    "! cat ID3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5决策树\n",
    "C4.5相对于ID3有以下的几个好处:\n",
    "* 使用了归一化的gain\n",
    "* 支持数值型特征,使用最大值与最小值的中值作为边界  \n",
    "目前不支持回归问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from scipy.stats import entropy\n",
      "from collections import Counter, defaultdict\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class C45:\n",
      "    def __init__(self):\n",
      "        self.root = None\n",
      "\n",
      "    def fit(self, x: pd.DataFrame, y: pd.Series):\n",
      "        \"\"\"\n",
      "        训练C4.5模型\n",
      "        :param x: pandas.DataFrame 训练特征数据\n",
      "        :param y: pandas.Series 目标数据\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        self.root = C45._split_data(x, y, x.columns)\n",
      "\n",
      "    def predict(self, x):\n",
      "        assert len(x.shape) == 2\n",
      "        result = []\n",
      "        for i in range(x.shape[0]):\n",
      "            result.append(C45._look_up(x.iloc[i, :], self.root))\n",
      "        return pd.Series(result)\n",
      "\n",
      "    @staticmethod\n",
      "    def _look_up(features, node):\n",
      "        label = None\n",
      "        if node.is_leaf:\n",
      "            label = node.label\n",
      "        else:\n",
      "            for item in node.nodes:\n",
      "                fe_name = node.split_feature\n",
      "                if node.threshold is None:\n",
      "                    if features[fe_name] == item.feature_value:\n",
      "                        label = C45._look_up(features, item)\n",
      "                else:\n",
      "                    if item.less and features[fe_name] <= node.threshold:\n",
      "                        label = C45._look_up(features, item)\n",
      "                    elif not item.less and features[fe_name] > node.threshold:\n",
      "                        label = C45._look_up(features, item)\n",
      "        if label is None:\n",
      "            label = \"Lookup Fail\"\n",
      "        return label\n",
      "\n",
      "    @staticmethod\n",
      "    def _split_data(x, y, remain_features):\n",
      "        if len(x) == 0:\n",
      "            return Node(True, \"Fail\")\n",
      "        if len(remain_features) == 0:\n",
      "            return Node(True, C45._most_common_label(y))\n",
      "        if len(y.unique()) == 1:\n",
      "            return Node(True, y.unique()[0])\n",
      "        best_feature, metric, threshold = C45._best_spilt_feature(x, y, remain_features)\n",
      "        node = Node(False, None, list(), split_feature=best_feature, threshold=threshold, metric=metric)\n",
      "        if node.threshold is not None:\n",
      "            less_mask = x[node.split_feature] <= threshold\n",
      "            greater_mask = x[node.split_feature] > threshold\n",
      "            less_node = C45._split_data(x[less_mask], y[less_mask],\n",
      "                                        list([fe for fe in remain_features if fe != best_feature]))\n",
      "            less_node.less = True\n",
      "            greater_node = C45._split_data(x[greater_mask], y[greater_mask],\n",
      "                                           list([fe for fe in remain_features if fe != best_feature]))\n",
      "            greater_node.less = False\n",
      "            node.nodes.append(less_node)\n",
      "            node.nodes.append(greater_node)\n",
      "        else:\n",
      "            for value in x[best_feature].unique():\n",
      "                examples = x[x[best_feature] == value]\n",
      "                target = y[x[best_feature] == value]\n",
      "                child_node = C45._split_data(examples, target,\n",
      "                                             list([fe for fe in remain_features if fe != best_feature]))\n",
      "                child_node.feature_value = value\n",
      "                node.nodes.append(child_node)\n",
      "        return node\n",
      "\n",
      "    @staticmethod\n",
      "    def _best_spilt_feature(x, y, remain_features):\n",
      "        if len(remain_features) == 0:\n",
      "            return remain_features[0]\n",
      "        best_feature = None\n",
      "        best_metric = None\n",
      "        best_threshold = None\n",
      "        for feature in remain_features:\n",
      "            if C45._is_discrete(x[feature]):\n",
      "                metric = C45._gain_ratio(x[feature], y)\n",
      "                if best_metric is None:\n",
      "                    best_metric = metric\n",
      "                    best_feature = feature\n",
      "                elif best_metric > metric:\n",
      "                    best_metric = metric\n",
      "                    best_feature = feature\n",
      "            else:\n",
      "                threshold = (np.min(x[feature]) + np.max(x[feature])) / 2\n",
      "                less = x[x[feature] <= threshold][feature]\n",
      "                greater = x[x[feature] > threshold][feature]\n",
      "                less_y = y[less.index]\n",
      "                greater_y = y[greater.index]\n",
      "                metric = len(less_y) / len(y) * C45._gain_ratio(less, less_y) + len(greater_y) / len(\n",
      "                    y) * C45._gain_ratio(greater, greater_y)\n",
      "                if best_metric is None:\n",
      "                    best_metric = metric\n",
      "                    best_feature = feature\n",
      "                    best_threshold = threshold\n",
      "                elif best_metric > metric:\n",
      "                    best_metric = metric\n",
      "                    best_feature = feature\n",
      "                    best_threshold = threshold\n",
      "        return best_feature, best_metric, best_threshold\n",
      "\n",
      "    @staticmethod\n",
      "    def _is_discrete(feature):\n",
      "        if str(feature.dtype) in [\"object\", \"str\"]:\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "    @staticmethod\n",
      "    def _gain_ratio(x, y):\n",
      "        fe_group_feature = defaultdict(list)\n",
      "        for fe, label in zip(x, y):\n",
      "            fe_group_feature[fe].append(label)\n",
      "        after_entropy = 0\n",
      "        before_entropy = entropy(np.asarray(list(Counter(y).values())) / len(y))\n",
      "        for fe in fe_group_feature:\n",
      "            labels = fe_group_feature[fe]\n",
      "            label_counter = Counter(labels)\n",
      "            label_counts = np.asarray(list(label_counter.values()))\n",
      "            pk = label_counts / np.sum(label_counts)\n",
      "            e = entropy(pk)\n",
      "            label_portion = len(fe_group_feature[fe]) / len(y)\n",
      "            after_entropy += e * label_portion\n",
      "        return (before_entropy - after_entropy) / (after_entropy + 1e-5)\n",
      "\n",
      "    @staticmethod\n",
      "    def _most_common_label(labels):\n",
      "        label_counter = Counter(labels)\n",
      "        return label_counter.most_common(1)[0][0]\n",
      "\n",
      "\n",
      "class Node:\n",
      "    def __init__(self, is_leaf, label, nodes=None, feature_value=None, threshold=None, metric=None,\n",
      "                 split_feature=None, less=None):\n",
      "        self.is_leaf = is_leaf\n",
      "        self.label = label\n",
      "        self.nodes = nodes\n",
      "        self.threshold = threshold\n",
      "        self.metric = metric\n",
      "        self.split_feature = split_feature\n",
      "        self.feature_value = feature_value\n",
      "        self.less = less\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    # dataset = pd.read_csv(\"xigua3.csv\")\n",
      "    # x = dataset[[\"色泽\", \"根蒂\", \"敲声\", \"纹理\", \"脐部\", \"触感\", \"密度\", \"含糖率\"]]\n",
      "    # y = dataset[\"好瓜\"]\n",
      "    from sklearn.model_selection import train_test_split\n",
      "    from sklearn.datasets import load_wine\n",
      "\n",
      "    x, y = load_wine(True)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
      "    model = C45()\n",
      "    model.fit(pd.DataFrame(X_train), pd.Series(y_train))\n",
      "    print(\"实际值\", y_test)\n",
      "    pred = model.predict(pd.DataFrame(X_test))\n",
      "    print(\"预测值\", pred)\n",
      "    print(\"acc:\", np.sum(pred.values == y_test) / len(pred))\n"
     ]
    }
   ],
   "source": [
    "! cat C45.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART回归树\n",
    "回归树生成过程与C4.5类似,回归树使用数据的均值作为预测值  \n",
    "通过最小化叶子结点数据的平方差来选择最有划分.  \n",
    "在选则划分点的时候理论上需要全量搜索所有可能的划分值,然后选择最优划分点,这一步会存在性能问题  \n",
    "划分结束可以通过阈值来设定和叶子节点的最小数据数量  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树划分时使用的度量有:  \n",
    "分类  \n",
    "* IG(information gain):信息增益\n",
    "* gain ratio:标准化的信息增益\n",
    "* gini impurity:基尼指数,两次抽样,类别不同的概率  \n",
    "\n",
    "回归  \n",
    "* MSE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
