{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics \n",
    "### Gini impurity\n",
    "应用于CART分类决策树,Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The Gini impurity can be computed by summing the probability $p_{i}$ of an item with label i being chosen times the probability $\\sum _{k\\neq i}p_{k}=1-p_{i}$ of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category.\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e3c72a8e705c2fddf2fd552241c579a6c146af7f\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain  \n",
    "Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy and information content from information theory.   \n",
    "熵定义:$\\mathrm {H} (T)=\\operatorname {I} _{E}\\left(p_{1},p_{2},...,p_{J}\\right)=-\\sum _{i=1}^{J}{p_{i}\\log _{2}p_{i}}$\n",
    "上述定义是对离散型随机变量的定义  \n",
    "where $\\displaystyle p_{1},p_{2},...$ $\\displaystyle p_{1},p_{2},...$are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree.  \n",
    "$\\displaystyle \\overbrace {IG(T,a)} ^{\\text{Information Gain}}=\\overbrace {\\mathrm {H} (T)} ^{\\text{Entropy (parent)}}-\\overbrace {\\mathrm {H} (T|a)} ^{\\text{Weighted Sum of Entropy (Children)}}$ $\\displaystyle =-\\sum _{i=1}^{J}p_{i}\\log _{2}{p_{i}}-\\sum _{a}{p(a)\\sum _{i=1}^{J}-\\Pr(i|a)\\log _{2}{\\Pr(i|a)}}$ $\\displaystyle =-\\sum _{i=1}^{J}p_{i}\\log _{2}{p_{i}}-\\sum _{a}{p(a)\\sum _{i=1}^{J}-\\Pr(i|a)\\log _{2}{\\Pr(i|a)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3  \n",
    "伪代码,是C4.5的前一个版本\n",
    "```\n",
    "ID3 (Examples, Target_Attribute, Attributes)\n",
    "    Create a root node for the tree\n",
    "    If all examples are positive, Return the single-node tree Root, with label = +.\n",
    "    If all examples are negative, Return the single-node tree Root, with label = -.\n",
    "    If number of predicting attributes is empty, then Return the single node tree Root,\n",
    "    with label = most common value of the target attribute in the examples.\n",
    "    Otherwise Begin\n",
    "        A ← The Attribute that best classifies examples.\n",
    "        Decision Tree attribute for Root = A.\n",
    "        For each possible value, vi, of A,\n",
    "            Add a new tree branch below Root, corresponding to the test A = vi.\n",
    "            Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "            If Examples(vi) is empty\n",
    "                Then below this new branch add a leaf node with label = most common target value in the examples\n",
    "            Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes – {A})\n",
    "    End\n",
    "    Return Root```  \n",
    "容易过拟合,不支持连续型特征,只能用于分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "from scipy.stats import entropy\n",
      "from collections import Counter, defaultdict\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class ID3:\n",
      "    def __init__(self):\n",
      "        self.root = None\n",
      "        self.features = None\n",
      "\n",
      "    def fit(self, x: pd.DataFrame, y: pd.Series):\n",
      "        \"\"\"\n",
      "        训练ID3模型\n",
      "        :param x: pandas.DataFrame 训练特征数据\n",
      "        :param y: pandas.Series 目标数据\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        self.features = x.columns\n",
      "        self.root = ID3._split_data(x, y, x.columns)\n",
      "\n",
      "    def predict(self, x):\n",
      "        assert len(x.shape) == 2\n",
      "        result = []\n",
      "        for i in range(x.shape[0]):\n",
      "            result.append(ID3._look_up(x.iloc[i, :], self.root))\n",
      "        return pd.Series(result)\n",
      "\n",
      "    @staticmethod\n",
      "    def _look_up(features, node):\n",
      "        if \"label\" in node:\n",
      "            return node[\"label\"]\n",
      "        else:\n",
      "            for n in node[\"nodes\"]:\n",
      "                fe_name = n[\"split_feature\"]\n",
      "                if features[fe_name] == n[\"feature_value\"]:\n",
      "                    return ID3._look_up(features, n)\n",
      "\n",
      "    @staticmethod\n",
      "    def _most_common_label(labels):\n",
      "        label_counter = Counter(labels)\n",
      "        return label_counter.most_common(1)[0][0]\n",
      "\n",
      "    @staticmethod\n",
      "    def _split_data(x: pd.DataFrame, y: pd.Series, remain_features):\n",
      "        node = dict()\n",
      "        # 类别唯一返回叶子节点\n",
      "        if len(y.unique()) == 1:\n",
      "            node[\"label\"] = y.unique()[0]\n",
      "            return node\n",
      "\n",
      "        # 无可分割属性，返回叶子节点以数量最多的类别作为分类结果\n",
      "        if len(remain_features) == 0:\n",
      "            node[\"label\"] = ID3._most_common_label(y)\n",
      "\n",
      "        best_feature, metric = ID3._best_split_feature(x, y, remain_features)\n",
      "        node[\"nodes\"] = list()\n",
      "\n",
      "        for value in x[best_feature].unique():\n",
      "            examples = x[x[best_feature] == value]\n",
      "            target = y[x[best_feature] == value]\n",
      "            child_node = ID3._split_data(examples, target, list([fe for fe in remain_features if fe != best_feature]))\n",
      "            child_node[\"feature_value\"] = value\n",
      "            child_node[\"split_feature\"] = best_feature\n",
      "            child_node[\"metric\"] = metric\n",
      "            node[\"nodes\"].append(child_node)\n",
      "        return node\n",
      "\n",
      "    @staticmethod\n",
      "    def _entropy(feature, label):\n",
      "        label_group_feature = defaultdict(list)\n",
      "        for fe, label in zip(feature, label):\n",
      "            label_group_feature[label].append(fe)\n",
      "        sum_entropy = 0\n",
      "        for label in label_group_feature:\n",
      "            feature = label_group_feature[label]\n",
      "            feature_counter = Counter(feature)\n",
      "            feature_counts = np.asarray(list(feature_counter.values()))\n",
      "            pk = feature_counts / np.sum(feature_counts)\n",
      "            e = entropy(pk)\n",
      "            label_portion = len(label_group_feature) / len(label)\n",
      "            sum_entropy += e * label_portion\n",
      "        return sum_entropy\n",
      "\n",
      "    @staticmethod\n",
      "    def _best_split_feature(x, y, remain_features):\n",
      "        if len(remain_features) == 0:\n",
      "            return remain_features[0]\n",
      "        metric_map = dict()\n",
      "        best_feature = None\n",
      "        best_metric = None\n",
      "        for feature in remain_features:\n",
      "            metric = ID3._entropy(x[feature], y)\n",
      "            metric_map[feature] = metric\n",
      "            if best_metric is None:\n",
      "                best_metric = metric\n",
      "                best_feature = feature\n",
      "            elif best_metric > metric:\n",
      "                best_metric = metric\n",
      "                best_feature = feature\n",
      "        return best_feature, best_metric\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    dataset = pd.read_csv(\"xigua3.csv\")\n",
      "    x = dataset[[\"色泽\", \"根蒂\", \"敲声\", \"纹理\", \"脐部\", \"触感\"]]\n",
      "    y = dataset[\"好瓜\"]\n",
      "    from sklearn.model_selection import train_test_split\n",
      "\n",
      "    model = ID3()\n",
      "    model.fit(x, y)\n",
      "    print(\"实际值\", y)\n",
      "    print(\"预测值\", model.predict(x))\n"
     ]
    }
   ],
   "source": [
    "! cat ID3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5决策树\n",
    "ID3决策树无法处理连续型变量,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
